model_list:
      - model_name: qwen3-embedding-0.6b
        litellm_params:
          model: "openai/@cf/qwen/qwen3-embedding-0.6b"
          api_base: "os.environ/CLOUDFLARE_WORKERS_API_BASE"
          api_key: "os.environ/CLOUDFLARE_WORKERS_API_KEY"
        model_info:
          mode: embedding
      - model_name: "cloudflare/@cf/meta/llama-2-7b-chat-fp16"
        litellm_params:
          model: "cloudflare/@cf/meta/llama-2-7b-chat-fp16"
          api_base: "os.environ/CLOUDFLARE_WORKERS_API_BASE"
          api_key: "os.environ/CLOUDFLARE_WORKERS_API_KEY"
        model_info:
          mode: chat
          max_input_tokens: 3072
          max_output_tokens: 3072
          input_cost_per_token: 0.000001923
          output_cost_per_token: 0.000001923
          max_tokens: 3072
      - model_name: "cloudflare/@cf/meta/llama-2-7b-chat-int8"
        litellm_params:
          model: "cloudflare/@cf/meta/llama-2-7b-chat-int8"
          api_base: "os.environ/CLOUDFLARE_WORKERS_API_BASE"
          api_key: "os.environ/CLOUDFLARE_WORKERS_API_KEY"
        model_info:
          mode: chat
          max_input_tokens: 2048
          max_output_tokens: 2048
          input_cost_per_token: 0.000001923
          output_cost_per_token: 0.000001923
          max_tokens: 2048
      - model_name: "cloudflare/@cf/mistral/mistral-7b-instruct-v0.1"
        litellm_params:
          model: "cloudflare/@cf/mistral/mistral-7b-instruct-v0.1"
          api_base: "os.environ/CLOUDFLARE_WORKERS_API_BASE"
          api_key: "os.environ/CLOUDFLARE_WORKERS_API_KEY"
        model_info:
          mode: chat
          max_input_tokens: 8192
          max_output_tokens: 8192
          input_cost_per_token: 0.000001923
          output_cost_per_token: 0.000001923
          max_tokens: 8192
      - model_name: "cloudflare/@hf/thebloke/codellama-7b-instruct-awq"
        litellm_params:
          model: "cloudflare/@hf/thebloke/codellama-7b-instruct-awq"
          api_base: "os.environ/CLOUDFLARE_WORKERS_API_BASE"
          api_key: "os.environ/CLOUDFLARE_WORKERS_API_KEY"
        model_info:
          mode: chat
          max_input_tokens: 4096
          max_output_tokens: 4096
          input_cost_per_token: 0.000001923
          output_cost_per_token: 0.000001923
          max_tokens: 4096
      - model_name: gpt-5.2-chat
        litellm_params:
          model: gpt-5.2-chat-latest
          api_key: "os.environ/openai_api_key_1"
          api_base: "os.environ/openai_base_url_1"
        model_info:
          mode: responses
          max_input_tokens: 128000
          max_output_tokens: 16384
          input_cost_per_token: 0.00000175
          output_cost_per_token: 0.000014
          cache_read_input_token_cost: 0.000000175
          cache_read_input_token_cost_priority: 0.00000035
          input_cost_per_token_priority: 0.0000035
          max_tokens: 16384
          output_cost_per_token_priority: 0.000028
          supported_endpoints:
            - "/v1/chat/completions"
            - "/v1/responses"
          supported_modalities:
            - "text"
            - "image"
          supported_output_modalities:
            - "text"
          supports_function_calling: true
          supports_native_streaming: true
          supports_parallel_function_calling: true
          supports_pdf_input: true
          supports_prompt_caching: true
          supports_reasoning: true
          supports_response_schema: true
          supports_system_messages: true
          supports_tool_choice: true
          supports_vision: true
      - model_name: gpt-5.2
        litellm_params:
          model: gpt-5.2
          api_key: "os.environ/openai_api_key_1"
          api_base: "os.environ/openai_base_url_1"
        model_info:
          mode: responses
          max_input_tokens: 400000
          max_output_tokens: 128000
          input_cost_per_token: 0.00000175
          output_cost_per_token: 0.000014
          cache_read_input_token_cost: 0.000000175
          cache_read_input_token_cost_priority: 0.00000035
          input_cost_per_token_priority: 0.0000035
          max_tokens: 128000
          output_cost_per_token_priority: 0.000028
          supported_endpoints:
            - "/v1/chat/completions"
            - "/v1/responses"
          supported_modalities:
            - "text"
            - "image"
          supported_output_modalities:
            - "text"
            - "image"
          supports_function_calling: true
          supports_native_streaming: true
          supports_parallel_function_calling: true
          supports_pdf_input: true
          supports_prompt_caching: true
          supports_reasoning: true
          supports_response_schema: true
          supports_system_messages: true
          supports_tool_choice: true
          supports_service_tier: true
          supports_vision: true
      - model_name: gpt-5.1
        litellm_params:
          model: gpt-5.1
          api_key: "os.environ/openai_api_key_1"
          api_base: "os.environ/openai_base_url_1"
        model_info:
          mode: responses
          max_input_tokens: 272000
          max_output_tokens: 128000
          input_cost_per_token: 0.00000125
          output_cost_per_token: 0.00001
          cache_read_input_token_cost: 0.000000125
          cache_read_input_token_cost_priority: 0.00000025
          input_cost_per_token_priority: 0.0000025
          max_tokens: 128000
          output_cost_per_token_priority: 0.00002
          supported_endpoints:
            - "/v1/chat/completions"
            - "/v1/responses"
          supported_modalities:
            - "text"
            - "image"
          supported_output_modalities:
            - "text"
            - "image"
          supports_function_calling: true
          supports_native_streaming: true
          supports_parallel_function_calling: true
          supports_pdf_input: true
          supports_prompt_caching: true
          supports_reasoning: true
          supports_response_schema: true
          supports_system_messages: true
          supports_tool_choice: true
          supports_service_tier: true
          supports_vision: true
      - model_name: gpt-5.1-codex
        litellm_params:
          model: gpt-5.1-codex
          api_key: "os.environ/openai_api_key_1"
          api_base: "os.environ/openai_base_url_1"
        model_info:
          mode: responses
          max_input_tokens: 272000
          max_output_tokens: 128000
          input_cost_per_token: 0.00000125
          output_cost_per_token: 0.00001
          cache_read_input_token_cost: 0.000000125
          cache_read_input_token_cost_priority: 0.00000025
          input_cost_per_token_priority: 0.0000025
          max_tokens: 128000
          output_cost_per_token_priority: 0.00002
          supported_endpoints:
            - "/v1/responses"
          supported_modalities:
            - "text"
            - "image"
          supported_output_modalities:
            - "text"
          supports_function_calling: true
          supports_native_streaming: true
          supports_parallel_function_calling: true
          supports_pdf_input: true
          supports_prompt_caching: true
          supports_reasoning: true
          supports_response_schema: true
          supports_system_messages: false
          supports_tool_choice: true
          supports_vision: true
      - model_name: gpt-5.1-codex-mini
        litellm_params:
          model: gpt-5.1-codex-mini
          api_key: "os.environ/openai_api_key_1"
          api_base: "os.environ/openai_base_url_1"
        model_info:
          mode: responses
          max_input_tokens: 272000
          max_output_tokens: 128000
          input_cost_per_token: 0.00000025
          output_cost_per_token: 0.000002
          cache_read_input_token_cost: 0.000000025
          cache_read_input_token_cost_priority: 0.000000045
          input_cost_per_token_priority: 0.00000045
          max_tokens: 128000
          output_cost_per_token_priority: 0.0000036
          supported_endpoints:
            - "/v1/responses"
          supported_modalities:
            - "text"
            - "image"
          supported_output_modalities:
            - "text"
          supports_function_calling: true
          supports_native_streaming: true
          supports_parallel_function_calling: true
          supports_pdf_input: true
          supports_prompt_caching: true
          supports_reasoning: true
          supports_response_schema: true
          supports_system_messages: false
          supports_tool_choice: true
          supports_vision: true
      - model_name: gpt-4.1
        litellm_params:
          model: gpt-4.1
          api_key: "os.environ/openai_api_key_1"
          api_base: "os.environ/openai_base_url_1"
        model_info:
          mode: chat
          max_input_tokens: 1047576
          max_output_tokens: 32768
          input_cost_per_token: 0.000002
          output_cost_per_token: 0.000008
          cache_read_input_token_cost: 0.0000005
          cache_read_input_token_cost_priority: 0.000000875
          input_cost_per_token_batches: 0.000001
          input_cost_per_token_priority: 0.0000035
          max_tokens: 32768
          supports_vision: true

litellm_settings:
  callbacks: ["prometheus"]
  prometheus_initialize_budget_metrics: true
  require_auth_for_metrics_endpoint: false
  cache: false
  cache_params:
    type: qdrant-semantic
    qdrant_api_base: http://qdrant.qdrant.svc.cluster.local:6333
    qdrant_api_key: os.environ/QDRANT_API_KEY
    qdrant_semantic_cache_embedding_model: text-embedding-3-large
    qdrant_collection_name: litellm_semantic_cache
    qdrant_quantization_config: binary
    qdrant_vector_size: 3072
    similarity_threshold: 0.8
    ttl: 600
    supported_call_types: ["acompletion", "atext_completion"]
  json_logs: true
  turn_off_message_logging: false
  redact_user_api_key_info: true
  enable_json_schema_validation: true
  set_verbose: true
  drop_params: true
  extra_spend_tag_headers:
    - "X-OpenWebUI-User-Id"
    - "X-OpenWebUI-User-Email"
    - "X-OpenWebUI-User-Name"

router_settings:
  routing_strategy: usage-based-routing-v2
  enable_pre_call_checks: false
  optional_pre_call_checks: ["responses_api_deployment_check"]
  num_retries: 3
  retry_after: 1
  allowed_fails: 3
  cooldown_time: 30
  allowed_fails_policy:
    AuthenticationErrorAllowedFails: 3
    TimeoutErrorAllowedFails: 6
    RateLimitErrorAllowedFails: 10000
    ContentPolicyViolationErrorAllowedFails: 6
    InternalServerErrorAllowedFails: 10
    BadRequestErrorAllowedFails: 1000
  max_fallbacks: 3
  enable_tag_filtering: true
  cache_responses: true
  redis_host: redis.litellm.svc.kube.pc-tips.se
  redis_port: "6379"

general_settings:
  store_model_in_db: true
  store_prompts_in_spend_logs: true
  enforce_user_param: false
  background_health_checks: false
  health_check_interval: 300
  health_check_details: true
  max_request_size_mb: 20
  max_response_size_mb: 25
  database_connection_pool_limit: 50
  database_connection_timeout: 60
  allow_requests_on_db_unavailable: false
  litellm_jwtauth:
    user_roles_jwt_field: "groups"
    user_allowed_roles: []
    sync_user_role_and_teams: true
    jwt_litellm_role_map:
      - jwt_role: "Litellm Admins"
        litellm_role: "proxy_admin"
      - jwt_role: "Litellm Users"
        litellm_role: "internal_user"
      - jwt_role: "internal_user_viewer"
        litellm_role: "internal_user"
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: customer
  generic_oauth:
    client_id: "os.environ/GENERIC_CLIENT_ID"
    client_secret: "os.environ/GENERIC_CLIENT_SECRET"
    redirect_url: "https://litellm.pc-tips.se/sso/callback"
    authorization_url: "https://sso.pc-tips.se/application/o/authorize/"
    token_url: "https://sso.pc-tips.se/application/o/token/"
    user_info_url: "https://sso.pc-tips.se/application/o/userinfo/"
    user_id_field: "sub"
    user_email_field: "email"
    user_name_field: "name"
    user_role_field: "groups"
    scope: "openid profile email groups"
