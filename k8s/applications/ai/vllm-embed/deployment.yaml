apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-embed
  labels:
    app: vllm-embed
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-embed
  template:
    metadata:
      labels:
        app: vllm-embed
    spec:
      containers:
        - name: vllm-openai
          image: ghcr.io/theepicsaxguy/vllm-cpu:v0.11.0-cfc13f2
          imagePullPolicy: IfNotPresent
          securityContext:
            capabilities:
              drop: ["ALL"]
          command: ["/bin/sh", "-c"]
          args:
            - >
              python3 -m vllm.entrypoints.openai.api_server
              --model Qwen/Qwen3-Embedding-0.6B
              --task embed
              --runner pooling
              --port 8000
              --host 0.0.0.0
              --max-model-len 4608
              --dtype float32
              --max-num-seqs 10
              --max-num-batched-tokens 32768
              --enforce-eager
              --disable-log-requests
              --api-key $VLLM_API_KEY
          env:
            # Reserve exactly 1GB for KV Cache (Enough for 4608 tokens)
            - name: VLLM_CPU_KVCACHE_SPACE
              value: "1"
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: app-vllm-embed-hf-token
                  key: HUGGING_FACE_HUB_TOKEN
            - name: VLLM_API_KEY
              valueFrom:
                secretKeyRef:
                  name: app-vllm-embed-api-key
                  key: VLLM_API_KEY
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - name: hf-cache
              mountPath: /root/.cache/huggingface
          resources:
            requests:
              cpu: "2000m"
              memory: "3Gi"
            limits:
              cpu: "4000m"
              memory: "4Gi"
      volumes:
        - name: hf-cache
          persistentVolumeClaim:
            claimName: vllm-model-cache